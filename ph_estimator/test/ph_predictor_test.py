import joblib
import numpy as np
import pandas as pd
import tensorflow as tf
import os

class PHPredictor:
    def __init__(self, model_dir='.'):
        """
        Initialize the pH predictor with saved models
        
        Args:
            model_dir: Directory containing the model files
        """
        # Check if neural network model exists
        if os.path.exists(os.path.join(model_dir, 'ph_nn_model.h5')):
            print("Loading neural network model...")
            self.model = tf.keras.models.load_model(os.path.join(model_dir, 'ph_nn_model.h5'))
            self.model_type = 'nn'
        else:
            print("Loading traditional ML model...")
            self.model = joblib.load(os.path.join(model_dir, 'ph_prediction_model.pkl'))
            self.model_type = 'ml'
        
        # Load scaler and feature names
        self.scaler = joblib.load(os.path.join(model_dir, 'feature_scaler.pkl'))
        self.feature_names = joblib.load(os.path.join(model_dir, 'feature_names.pkl'))
        
        print(f"Model loaded successfully. Type: {self.model_type}")
        print(f"Model expects these features: {self.feature_names}")
    
    def _create_features(self, chla, temp, do):
        """
        Create all necessary features from basic input parameters
        
        Args:
            chla: Chlorophyll-a concentration in mg/L
            temp: Water temperature in Celsius
            do: Dissolved oxygen in mg/L
            
        Returns:
            DataFrame with all required features
        """
        # Create basic feature set
        features = pd.DataFrame({
            'chla': [chla], 
            'temp': [temp], 
            'do': [do]
        })
        
        # If model only uses basic features, return them
        if len(self.feature_names) == 3 and set(self.feature_names) == {'chla', 'temp', 'do'}:
            return features
        
        # Otherwise, generate all the features used during training
        # Temperature and DO interaction
        features['temp_do_interaction'] = features['temp'] * features['do']
        
        # Chlorophyll and temperature interaction
        features['chla_temp_interaction'] = features['chla'] * features['temp']
        
        # Chlorophyll and dissolved oxygen interaction
        features['chla_do_interaction'] = features['chla'] * features['do']
        
        # Ratio features
        features['do_temp_ratio'] = features['do'] / (features['temp'] + 1e-5)
        features['chla_do_ratio'] = features['chla'] / (features['do'] + 1e-5)
        
        # Squared terms
        features['chla_squared'] = features['chla'] ** 2
        features['temp_squared'] = features['temp'] ** 2
        features['do_squared'] = features['do'] ** 2
        
        # Log transformations
        features['chla_log'] = np.log(features['chla'] + 1e-5)
        features['temp_log'] = np.log(features['temp'] + 1e-5)
        features['do_log'] = np.log(features['do'] + 1e-5)
        
        # If feature set includes polynomial features, they were generated from this base set
        # We'll only return the columns we know about - the scaler and model will handle the rest
        common_features = [col for col in features.columns if col in self.feature_names]
        return features[common_features]
    
    def predict_ph(self, chla, temp, do):
        """
        Predict pH from water quality parameters
        
        Args:
            chla: Chlorophyll-a concentration in mg/L
            temp: Water temperature in Celsius
            do: Dissolved oxygen in mg/L
            
        Returns:
            Predicted pH value
        """
        # Input validation
        if chla < 0 or temp < 0 or do < 0:
            raise ValueError("Input parameters must be non-negative")
            
        # Create features based on the known feature names list
        features_df = self._create_features(chla, temp, do)
        
        # Ensure the feature order matches the training order
        # Note: This assumes _create_features returns columns potentially out of order
        # or a subset, and self.feature_names holds the definitive list and order.
        features_df = features_df[self.feature_names] 

        # Check if the scaler expects more features than we currently have
        # This suggests polynomial features were used during training but not explicitly listed
        # in feature_names or generated by _create_features in the exact same way.
        if hasattr(self.scaler, 'n_features_in_') and self.scaler.n_features_in_ != features_df.shape[1]:
            print(f"Warning: Scaler expects {self.scaler.n_features_in_} features, but generated {features_df.shape[1]}. Applying PolynomialFeatures.")
            # We need the original basic features to generate polynomial ones correctly
            basic_features_df = pd.DataFrame({'chla': [chla], 'temp': [temp], 'do': [do]})
            
            from sklearn.preprocessing import PolynomialFeatures
            # Determine the degree based on the expected number of features.
            # This is an estimation and might be incorrect if the original features weren't just the 3 basic ones.
            # For 119 features from 3 base features, degree 4 with interaction_only=False seems likely.
            # n_output_features = (n_features + d)! / (d! * n_features!) - 1 (if include_bias=False)
            # For n=3, d=4: (3+4)! / (4! * 3!) - 1 = 7! / (4! * 3!) - 1 = 5040 / (24 * 6) - 1 = 5040 / 144 - 1 = 35 - 1 = 34 (Incorrect)
            # Let's try degree 4 with interaction_only=False, include_bias=False
            # Need to figure out the correct degree/combination that yields 119 features from the base or the 14 generated.
            # It's more likely the 119 features came from the 14 generated features.
            # Let's assume degree 2 for the 14 features for now as a guess.
            # (14+2)! / (2! * 14!) - 1 = 16! / (2 * 14!) - 1 = (16*15)/2 - 1 = 120 - 1 = 119. This matches!
            poly_degree = 2 
            print(f"Applying PolynomialFeatures(degree={poly_degree}) to the {features_df.shape[1]} generated features.")
            poly = PolynomialFeatures(degree=poly_degree, include_bias=False)
            
            # Fit and transform using the DataFrame to preserve feature names if possible,
            # though PolynomialFeatures often drops them. Convert to numpy array for transform.
            features_poly = poly.fit_transform(features_df)
            
            # Check if the number of features now matches the scaler's expectation
            if features_poly.shape[1] != self.scaler.n_features_in_:
                 raise ValueError(f"PolynomialFeatures(degree={poly_degree}) generated {features_poly.shape[1]} features, but scaler expects {self.scaler.n_features_in_}. Check PolynomialFeatures degree or training process.")
            
            # Use the polynomial features for scaling
            features_to_scale = features_poly
            # Get feature names after polynomial transformation if possible (for potential debugging)
            # poly_feature_names = poly.get_feature_names_out(features_df.columns)
            # print(f"Generated {len(poly_feature_names)} polynomial feature names.")

        else:
             # Use the originally generated features if counts match
             features_to_scale = features_df.values # Convert to numpy array for scaler

        # Scale features
        # The UserWarning about feature names suggests the scaler was fitted on a numpy array.
        # Pass a numpy array to transform to avoid the warning.
        features_scaled = self.scaler.transform(features_to_scale)
        
        # Make prediction
        if self.model_type == 'nn':
            # Ensure input shape matches NN expectation if needed (e.g., adding batch dimension)
            if features_scaled.ndim == 1:
                 features_scaled = np.expand_dims(features_scaled, axis=0)
            elif features_scaled.shape[0] != 1: # Should be (1, n_features)
                 pass # Already has batch dim potentially? Check model input spec.

            ph_prediction = self.model.predict(features_scaled)[0][0]
        else: # Assumes scikit-learn like model
             ph_prediction = self.model.predict(features_scaled)[0]
        
        return ph_prediction

def predict_ph(chla, temp, do):
    """
    Simple function to predict pH from water quality parameters
    
    Args:
        chla: Chlorophyll-a concentration in mg/L
        temp: Water temperature in Celsius
        do: Dissolved oxygen in mg/L
        
    Returns:
        Predicted pH value
    """
    predictor = PHPredictor()
    return predictor.predict_ph(chla, temp, do)

# Example usage
if __name__ == "__main__":
    # Example inputs
    chla_value = 2.5  # mg/L
    temp_value = 22.3  # Celsius
    do_value = 8.7  # mg/L
    
    # Create predictor
    predictor = PHPredictor()
    
    # Make prediction
    predicted_ph = predictor.predict_ph(chla_value, temp_value, do_value)
    
    print(f"Water Quality Parameters:")
    print(f"  Chlorophyll-a: {chla_value} mg/L")
    print(f"  Temperature:   {temp_value} °C")
    print(f"  Dissolved O2:  {do_value} mg/L")
    print(f"Predicted pH:    {predicted_ph:.2f}")
    
    # Create a grid of predictions for visualization
    print("\nGenerating pH prediction surface...")
    
    # Create ranges for parameters
    temp_range = np.linspace(10, 30, 5)  # 5 temperature points from 10-30°C
    do_range = np.linspace(4, 12, 5)     # 5 DO points from 4-12 mg/L
    
    print("\npH Prediction Grid (Rows: Temperature, Columns: DO, Fixed Chla=2.5):")
    print("     ", end="")
    for do in do_range:
        print(f"{do:5.1f}", end=" ")
    print()
    
    for temp in temp_range:
        print(f"{temp:4.1f} ", end="")
        for do in do_range:
            predicted = predictor.predict_ph(chla_value, temp, do)
            print(f"{predicted:5.2f}", end=" ")
        print()